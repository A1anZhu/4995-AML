{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607",
   "metadata": {
    "id": "d9bcdbd2-3401-41ad-a83f-830e9346e607"
   },
   "source": [
    "# **Applied Machine Learning Homework 5**\n",
    "**Due 12 Dec,2022 (Monday) 11:59PM EST**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed43df68",
   "metadata": {},
   "source": [
    "Your Name: Liwen Zhu\n",
    "\n",
    "Your UNI: lz2512"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df26be-5638-4b0d-a252-4437eb76aa46",
   "metadata": {
    "id": "70df26be-5638-4b0d-a252-4437eb76aa46"
   },
   "source": [
    "### Natural Language Processing\n",
    "We will train a supervised model to predict if a movie has a positive or a negative review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d",
   "metadata": {
    "id": "2e0d9a19-25ea-4490-b0e8-7909bcdc3d9d"
   },
   "source": [
    "####  **Dataset loading & dev/test splits**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7",
   "metadata": {
    "id": "fafa37c4-c8fc-4697-9bbe-11539d710bf7"
   },
   "source": [
    "**1.0) Load the movie reviews dataset from NLTK library**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f4ce405-237b-42d2-9c81-25ff28deaf4a",
    "outputId": "46c1d8f9-5493-4df8-e40f-b62cf7bfbbfc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /Users/alanzhu/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/alanzhu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/alanzhu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"movie_reviews\")\n",
    "import pandas as pd\n",
    "from nltk.corpus import twitter_samples \n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "stop = stopwords.words('english')\n",
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b220f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "Nmt1nVaNpkKy",
   "metadata": {
    "id": "Nmt1nVaNpkKy"
   },
   "outputs": [],
   "source": [
    "negative_fileids = movie_reviews.fileids('neg')\n",
    "positive_fileids = movie_reviews.fileids('pos')\n",
    "\n",
    "pos_document = [(' '.join(movie_reviews.words(file_id)),category) for file_id in movie_reviews.fileids() for category in movie_reviews.categories(file_id) if category == 'pos']\n",
    "neg_document = [(' '.join(movie_reviews.words(file_id)),category) for file_id in movie_reviews.fileids() for category in movie_reviews.categories(file_id) if category == 'neg']\n",
    "\n",
    "# List of postive and negative reviews\n",
    "pos_list = [pos[0] for pos in pos_document]\n",
    "neg_list = [neg[0] for neg in neg_document]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-B4xT6L714l2",
   "metadata": {
    "id": "-B4xT6L714l2"
   },
   "source": [
    "**1.1) Make a data frame that has reviews and its label**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "zFkLGJ5p118y",
   "metadata": {
    "id": "zFkLGJ5p118y"
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "movie = pd.DataFrame(pos_document+neg_document,columns=[\"Review\",\"Label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eWdYSe2xxxCz",
   "metadata": {
    "id": "eWdYSe2xxxCz"
   },
   "source": [
    "**1.2 look at the class distribution of the movie reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2be173e3",
   "metadata": {
    "id": "2be173e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    1000\n",
       "neg    1000\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# code here\n",
    "movie[\"Label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12eae071-fd8a-4a46-9958-0525c635fd88",
   "metadata": {
    "id": "12eae071-fd8a-4a46-9958-0525c635fd88"
   },
   "source": [
    "**1.3) Create a development & test split (80/20 ratio):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a09323b",
   "metadata": {
    "id": "1a09323b"
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "X_dev,X_test,y_dev,y_test = train_test_split(movie[\"Review\"],movie[\"Label\"],test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d",
   "metadata": {
    "id": "32b23398-e80e-4624-b89e-c02fabfd3f8d"
   },
   "source": [
    "#### **Data preprocessing**\n",
    "We will do some data preprocessing before we tokenize the data. We will remove `#` symbol, hyperlinks, stop words & punctuations from the data. You may use `re` package for this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310",
   "metadata": {
    "id": "f89d9d69-1640-4583-a7b7-7ec04ccf3310"
   },
   "source": [
    "**1.4) Replace the `#` symbol with '' in every review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5db4dd6d-e775-49d3-96e1-57620c042d46",
   "metadata": {
    "id": "5db4dd6d-e775-49d3-96e1-57620c042d46"
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "X_dev = X_dev.str.replace('#','')\n",
    "X_test = X_test.str.replace('#','')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe",
   "metadata": {
    "id": "24c4caa8-d71d-46a8-8859-a8e85c56acfe"
   },
   "source": [
    "**1.5) Replace hyperlinks with '' in every review**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff5a7411-df49-427b-adef-5e8e63224db0",
   "metadata": {
    "id": "ff5a7411-df49-427b-adef-5e8e63224db0"
   },
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492ae463-b611-4292-9ad2-b778856bf8bc",
   "metadata": {
    "id": "492ae463-b611-4292-9ad2-b778856bf8bc"
   },
   "source": [
    "**1.6) Remove all stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961d73fd-a662-46f2-85a2-83bf6b978189",
   "metadata": {
    "id": "961d73fd-a662-46f2-85a2-83bf6b978189"
   },
   "outputs": [],
   "source": [
    "# code here\n",
    "for word in stop:\n",
    "    X_dev = X_dev.str.replace(' '+word+' ',' ').replace(' '+word,' ').replace(word+' ',' ')\n",
    "    X_test = X_test.str.replace(' '+word+' ',' ').replace(' '+word,' ').replace(word+' ',' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9",
   "metadata": {
    "id": "169bf8ad-f7ba-4e67-a1a0-92fcdd193ab9"
   },
   "source": [
    "**1.7) Remove all punctuations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e",
   "metadata": {
    "id": "774743e0-8cf0-4dbb-a6fa-006ff076bb9e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/kn5sds75613b8_2rx88jfxf80000gn/T/ipykernel_81963/3922321045.py:3: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  X_dev = X_dev.str.replace(punctuation,' ')\n",
      "/var/folders/b0/kn5sds75613b8_2rx88jfxf80000gn/T/ipykernel_81963/3922321045.py:4: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  X_test = X_test.str.replace(punctuation,' ')\n"
     ]
    }
   ],
   "source": [
    "# code here\n",
    "for punctuation in string.punctuation:\n",
    "    X_dev = X_dev.str.replace(punctuation,' ')\n",
    "    X_test = X_test.str.replace(punctuation,' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f1af18-0c07-4ffb-994e-daead4740a53",
   "metadata": {
    "id": "b2f1af18-0c07-4ffb-994e-daead4740a53"
   },
   "source": [
    "**1.8) Apply stemming on the development & test datasets using Porter algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c84a52f6-a62a-4033-8d1d-239ff6904248",
   "metadata": {
    "id": "c84a52f6-a62a-4033-8d1d-239ff6904248"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "def stemSentence(sentece):\n",
    "    porter = PorterStemmer()\n",
    "    token_words = word_tokenize(sentece)\n",
    "    stem_sentence = [porter.stem(word) for word in token_words]\n",
    "    return \" \".join(stem_sentence)\n",
    "\n",
    "for index,sentence in X_dev.iteritems():\n",
    "    X_dev[index] = stemSentence(sentence)\n",
    "for index,sentence in X_test.iteritems():\n",
    "    X_test[index] = stemSentence(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687e23ef-dafd-4183-b2f1-86089e281dd8",
   "metadata": {
    "id": "687e23ef-dafd-4183-b2f1-86089e281dd8"
   },
   "source": [
    "#### **Model training**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef",
   "metadata": {
    "id": "0c40fa44-01ad-4788-98b9-9c8f0c1252ef"
   },
   "source": [
    "**1.9) Create bag of words features for each review in the development dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a",
   "metadata": {
    "id": "c17c6b99-9dfb-4d30-9e03-d596a9da880a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0009f', '007', '03', '04', '05', '05425', '10', '100']\n",
      "['hype', 'hyper', 'hyperact', 'hyperbol', 'hyperdr', 'hyperjump', 'hyperkinet', 'hypernatur', 'hyperr', 'hypersleep', 'hyperspe', 'hyperviol', 'hypnosi', 'hypnot', 'hypnotherapist', 'hypnotis', 'hypnotist', 'hypochondriac', 'hypocrisi', 'hypocrit']\n",
      "['00', 'dah', 'hype', 'painter', 'sturm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alanzhu/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "vector = CountVectorizer(stop_words = 'english')\n",
    "X_dev_bow = vector.fit_transform(X_dev)\n",
    "feature_names = vector.get_feature_names()\n",
    "print(feature_names[:10])\n",
    "print(feature_names[10000:10020])\n",
    "print(feature_names[::5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e",
   "metadata": {
    "id": "4baf65cd-019b-4ff4-b93c-3ca8cfffca8e"
   },
   "source": [
    "**1.10) Train a Logistic Regression model on the development dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3433a6b0-408d-462e-9072-3495b21bc97b",
   "metadata": {
    "id": "3433a6b0-408d-462e-9072-3495b21bc97b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alanzhu/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "lr = LogisticRegression().fit(X_dev_bow,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340",
   "metadata": {
    "id": "1c16c6f6-7ab2-4d7a-b9dc-098a72381340"
   },
   "source": [
    "**1.11) Create TF-IDF features for each review in the development dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510",
   "metadata": {
    "id": "7b417843-ffc4-4614-b2ef-964f8ec3e510"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['00', '000', '0009f', '007', '03', '04', '05', '05425', '10', '100']\n",
      "['humil', 'humili', 'hummabl', 'hummana', 'hummer', 'hummingbird', 'hummm', 'humor', 'humorist', 'humorless', 'humour', 'humourless', 'hump', 'humpalot', 'humphrey', 'humphri', 'humve', 'hun', 'hunch', 'hunchback']\n",
      "['00', 'cyborsuit', 'humil', 'overfil', 'straddl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alanzhu/opt/anaconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "vector_tf = TfidfVectorizer()\n",
    "X_dev_tf = vector_tf.fit_transform(X_dev)\n",
    "feature_names_tf = vector_tf.get_feature_names()\n",
    "print(feature_names_tf[:10])\n",
    "print(feature_names_tf[10000:10020])\n",
    "print(feature_names_tf[::5000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427",
   "metadata": {
    "id": "ea3c9776-aad9-4eda-b3c2-d9f6b3e52427"
   },
   "source": [
    "**1.12) Train the Logistic Regression model on the development dataset with TF-IDF features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce",
   "metadata": {
    "id": "b8c7fe8b-61de-4daa-a338-74295a4902ce"
   },
   "outputs": [],
   "source": [
    "#code here\n",
    "lr_tf = LogisticRegression().fit(X_dev_tf,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92",
   "metadata": {
    "id": "ab0129e7-a0ea-473e-9ad1-667b44a13a92"
   },
   "source": [
    "**1.13) Compare the performance of the two models on the test dataset. Explain the difference in results obtained?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ca644f9",
   "metadata": {
    "id": "7ca644f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The performance of bag of words is 0.835\n",
      "The performance of tf-idf is 0.8225\n",
      "The bag of words weights more on the high frequency word. On the other hand, the TF-IDF model weights more on the unique word. The two models weight words differetly, which causes the difference in results.\n"
     ]
    }
   ],
   "source": [
    "#code here\n",
    "X_test_lr = vector.transform(X_test)\n",
    "X_test_tf = vector_tf.transform(X_test)\n",
    "print(f\"The performance of bag of words is {lr.score(X_test_lr,y_test)}\")\n",
    "print(f\"The performance of tf-idf is {lr_tf.score(X_test_tf,y_test)}\")\n",
    "print(\"The bag of words weights more on the high frequency word. \\\n",
    "On the other hand, the TF-IDF model weights more on the unique word. \\\n",
    "The two models weight words differetly, which causes the difference in results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fa2bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
